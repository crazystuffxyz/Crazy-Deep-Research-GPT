import{Ollama}from"ollama";import{getFetchResults,getSearchResults}from"./interactWithInternet.js";import{generateCitation}from"./extractCitation.js";import pLimit from"p-limit";const ollama=new Ollama,MAX_SERP_GENERATION_RETRIES=9,MAX_LINKS_PER_QUERY=3,FETCH_CONCURRENCY=3,DEFAULT_LITE_RESEARCH_HELPER_MODEL="qwen2.5vl:3b",DEFAULT_LITE_RESEARCH_PAPER_MODEL="qwen2.5vl:3b";function parse(e){return e.startsWith("<think>")?(1==(e=(e=e.replace("<think>","")).split("</think>")).length&&e.push(""),e):["",e]}function isValidLink(e,t){if(t||(t=new Set),!e||"string"!=typeof e||t.has(e)||e.endsWith(".pdf"))return!1;try{const t=new URL(e),a=t.hostname.toLowerCase();return![".pdf",".doc",".docx",".ppt",".pptx",".xls",".xlsx",".zip",".rar",".exe",".dmg",".pkg",".mp3",".mp4",".mov",".avi",".iso"].some((e=>t.pathname.toLowerCase().endsWith(e)))&&(a.endsWith(".org")||a.endsWith(".gov")||a.endsWith(".edu")||a.endsWith(".com")||a.endsWith(".net")||a.includes("wikipedia.org")||a.includes("github.io")||!a.match(/\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/))}catch(e){return!1}}function parseSerpQueries(e){if(!e||"string"!=typeof e)return[];const t=(e=parse(e)[1]).split("\n"),a=[],n=/^\s*(?:searchUp|SearchUp|searchup)\("([^"]+)"\)\s*$/i;for(const e of t){const t=e.trim().match(n);t&&t[1]&&"query"!==t[1].toLowerCase()&&t[1].length>3&&a.push(t[1])}return a}async function generateSerpQueries(e,t,a,n,r,i){let s=0;const o=t.slice(-3).map((e=>`${e.role}: ${e.content.substring(0,150)}...`)).join("\n");for(;s<=r;){s++,n({status:"Generating SERP Queries",detail:`Attempt ${s}/${r+1} for goal: "${e.substring(0,50)}..."`,isFinal:!1});try{const t={model:i,messages:[{role:"system",content:`Based on the research goal and recent conversation history, generate exactly ${a} unique and diverse SERP queries. Format each as: searchUp("query"). YOu must follow the format no matter what. Only searchUp("query") and nothing else. Please use that exact format and nothing else. \n\nConversation History:\n${o}`},{role:"user",content:`Research Goal: ${e}`}],stream:!1,options:{temperature:.7}},l=await ollama.chat(t),u=parse(l.message?.content||"")[1],c=parseSerpQueries(u);if(c.length>=a){const e=[...new Set(c)];return n({status:"Generated SERP Queries",detail:`Found ${e.length} unique queries: ${e.slice(0,a).map((e=>`"${e}"`)).join(", ")}`,isFinal:!1}),e.slice(0,a)}if(n({status:"Warning",detail:`AI did not generate enough valid SERP queries (Attempt ${s}, got ${c.length}, need ${a}). Raw: ${u.substring(0,100)}...`,isFinal:!1}),s>r&&c.length>0)return n({status:"Generated SERP Queries (Partial)",detail:`Using ${c.length} queries after retries.`,isFinal:!1}),c.slice(0,a);if(s>r)throw new Error("No valid SERP queries generated after retries.")}catch(e){if(n({status:"Error Generating SERP",error:e.message,detail:`Attempt ${s}`,isFinal:!1}),s>r)throw new Error(`Failed to generate SERP queries after ${s} attempts: ${e.message}`)}}throw new Error("Failed to generate SERP queries within retry limits.")}async function generateCitationAndSummary(e,t,a,n,r,i,s,o){r({status:"Generating Citation & Summary",detail:`${i} For URL: ${e} (HeavyDuty: ${o})`,isFinal:!1});try{const n=1e5,c=t.length>n?t.substring(0,n)+"...":t;if(c.startsWith("Error:")||c.length<150)return r({status:"Skipping Citation/Summary",detail:`${i} Content too short or error: ${e}`,isFinal:!1}),null;let h=`Analyze the provided text content in relation to the overall research goal: "${a}".\nFocus on extracting key facts, arguments, and data directly relevant to this goal.\nOutput *only* the summary text. Do not add any other explanations or surrounding text.`;h+=o?"\n\nThis is a HEAVY DUTY research task. The summary must be extremely comprehensive, detailed, and analytical.\nAim for at least 10-15 well-developed paragraphs, thoroughly exploring the nuances of the provided content in relation to the research goal.\nEach paragraph MUST have at least 15-20 sentences, aim for the most that you can. \nSynthesize information thoughtfully. Do not just list facts; explain their significance and interconnections.\nThe summary should be suitable for inclusion in a PhD-level research paper.":"\nProvide a detailed summary of 5 paragraphs with at least 5-7 sentences. It should be in an essay format, and be around high school level. Each paragraph must represent different aspects of the data given and how it answers the overall research goal.";const d=[{role:"system",content:h},{role:"user",content:`URL: ${e}\n\nContent:\n${c}`}],m=generateCitation(e).catch((t=>(console.error(`${i} Error calling generateCitation for ${e}:`,t),{_internal_error_flag_:!0,message:t.message||"Unknown error in generateCitation"}))),g=ollama.chat({model:s,messages:d,stream:!1,options:{temperature:o?.4:.5}});var[l,u]=await Promise.all([m,g]);u=parse(u)[1];let p="",f=null;l&&l._internal_error_flag_?(f=`Citation generation failed internally: ${l.message}`,p="Error generating citation due to internal failure."):l&&l.citations&&l.citations.hasOwnProperty("MLA")&&"string"==typeof l.citations.MLA?(p=l.citations.MLA.trim(),p||(f="MLA citation found but was empty.",p="Automated MLA citation was empty.")):(f=l?l.citations?l.citations.hasOwnProperty("MLA")?"string"!=typeof l.citations.MLA?`MLA citation value was not a string (type: ${typeof l.citations.MLA}).`:"Unknown issue processing citation data structure.":"MLA format not found in the generated citations (missing 'MLA' key).":"Citation data from service is missing the 'citations' block.":"Citation generation service returned no data (null or undefined).",p="Automated citation generation failed (structure/content issue)."),f&&r({status:"Warning Generating Citation",detail:`${i} URL: ${e}. ${f}`,isFinal:!1});const y=u.message?.content?.trim()||"",$=!f&&p,v=y.length>(o?100:20);if($&&v)return{citation:p,summary:y};{let t=[];return $||t.push(f||p||"Citation generation failed."),y?v||t.push(`Summary was too short (got ${y.length} chars, needed > ${o?100:20}).`):t.push("Summary was empty."),r({status:"Warning Generating Citation/Summary Parts",detail:`${i} URL: ${e}. ${t.join(" ")} Citation: "${(p||"").substring(0,50)}...", Summary: "${y.substring(0,50)}..."`,isFinal:!1}),{citation:p||"Automated citation generation failed.",summary:y||(o?"Automated comprehensive summary generation failed.":"Automated summary generation failed.")}}}catch(t){return console.error(`${i} General error in generateCitationAndSummary for ${e}:`,t),r({status:"Error Generating Citation/Summary",detail:`${i} URL: ${e}`,error:t.message,isFinal:!1}),{citation:"Error generating citation.",summary:"Error generating summary."}}}async function processSingleSerpQuery(e,t,a,n,r,i,s,o,l,u){i({status:"Processing SERP Query",detail:`${s} Query: "${e}"`,isFinal:!1});let c=[];try{if(c=await getSearchResults(e),!c||0===c.length)return i({status:"No Search Results",detail:`${s} No results for "${e}"`,isFinal:!1}),[];i({status:"Got Search Results",detail:`${s} Found ${c.length} results for "${e}"`,isFinal:!1})}catch(t){return i({status:"Error Searching",detail:`${s} Query: "${e}"`,error:t.message,isFinal:!1}),[]}const h=[];for(const e of c)if(isValidLink(e.link,n)&&(h.push(e.link),h.length>=MAX_LINKS_PER_QUERY))break;if(h.forEach((e=>n.add(e))),l&&h.length>0&&h.forEach((e=>{i({heavyDutyResearchUpdate:{visitedUrl:e,statusDetail:`Marked ${e} as visited.`},isFinal:!1,heavyDutyResearchId:l})})),0===h.length)return i({status:"No New Valid Links",detail:`${s} No new usable links found for "${e}".`,isFinal:!1}),[];i({status:"Fetching & Processing Content",detail:`${s} Processing ${h.length} link(s) for "${e}"`,isFinal:!1});const d=h.map(((e,n)=>r((async()=>{const r=`${s} Link ${n+1}/${h.length}`;try{i({status:"Fetching Content",detail:`${r} Fetching: ${e}`,isFinal:!1});const n=await getFetchResults(e);if("string"!=typeof n||n.startsWith("Error:"))return i({status:"Error Fetching Content",detail:`${r} Failed: ${e}`,error:n||"Unknown fetch error",isFinal:!1}),null;i({status:"Fetched Content",detail:`${r} Success: ${e}`,isFinal:!1});const s=await generateCitationAndSummary(e,n,t,a,i,r,o,u);if(s&&s.citation&&s.summary&&s.summary.length>20){const t={url:e,citation:s.citation,summary:s.summary};return l&&i({heavyDutyResearchUpdate:{processedDataChunk:t,statusDetail:`Processed ${e}`},isFinal:!1,heavyDutyResearchId:l}),t}if(s){i({status:"Warning",detail:`${r} Partial data from citation/summary for ${e}. Citation: ${s.citation?.substring(0,30)}... Summary: ${s.summary?.substring(0,30)}...`,isFinal:!1});const t={url:e,citation:s.citation||"Automated citation failed.",summary:s.summary||"Automated summary failed."};return l&&i({heavyDutyResearchUpdate:{processedDataChunk:t,statusDetail:`Processed (partially) ${e}`},isFinal:!1,heavyDutyResearchId:l}),t}return null}catch(t){return i({status:"Error Processing Link",detail:`${r} URL: ${e}`,error:t.message,isFinal:!1}),null}}))));return(await Promise.all(d)).filter((e=>null!==e))}async function generateFinalReport(e,t,a,n,r,i,s,o){let l="",u="";const c=o?"at least 15":"5-7";if(e.length>0){i({status:"Generating Final Report",detail:`Synthesizing paper from ${e.length} summaries. Target: ${c} paragraphs. Heavy Duty: ${o}`,isFinal:!1});try{const n=e.map(((e,t)=>`Source ${t+1} (URL: ${e.url}):\nSummary: ${e.summary}\nCitation: ${e.citation}`)).join("\n\n---\n\n"),r=a.slice(-3).map((e=>`${e.role}: ${e.content.substring(0,150)}...`)).join("\n");let u;u=o?`You are a meticulous research assistant. Based *only* on the provided research summaries and their citations, and considering the original query and recent conversation, write a comprehensive PhD-level research paper.\nOriginal Query: "${t}"\nRecent Conversation Context:\n${r}\n\nThis is a HEAVY DUTY research task. The paper must be extremely detailed, meticulous, and PhD-level in quality. It should be structured as if it's an actual PhD research paper and must contain ${c} well-developed paragraphs. Focus on deep analysis, synthesis of information, and critical evaluation. Each paragraph must contain at least 15 sentences of meticulous research, and must contain thorough in-text citations.\n\n- Structure:\n    - Introduction: Clearly state the research problem/question derived from the original query, outline the scope of the paper, and provide a thesis statement or overview of main findings.\n    - Literature Review (implicitly, by synthesizing sources): Discuss the provided summaries, grouping them by theme or argument. Critically analyze the findings from each source in relation to others.\n    - Methodology (if applicable, or inferred from the task of synthesizing summaries): Briefly explain how the information was gathered/synthesized (from the provided summaries).\n    - Results/Discussion: Present the synthesized findings in a structured manner. Each of the ${c.startsWith("at least ")?c.substring(9):"required"} paragraphs should explore a specific aspect in depth, supported by evidence from the summaries. Analyze, interpret, and discuss the significance of these findings.\n    - Conclusion: Summarize the main arguments and findings. Discuss limitations based *only* on the provided material. Suggest potential implications or areas for future research (if derivable from summaries).\n- Synthesize findings from the summaries into a coherent narrative.\n- Do *not* refer to "Source 1", "Source 2", etc. Integrate information smoothly, as if you are the author who has read these sources. Information should be attributed by its content aligning with the provided summaries. In fact, sources must also be cited as in text citations.\n- Ensure extensive use of information clearly tied to the provided summaries. The language should be academic, formal, and precise.\n- The final output should be ONLY the research paper text. A separate list of works cited will be compiled later using the provided citation data. Do not include a "References" or "Bibliography" section in your response.\n- Adhere strictly to the information within the provided summaries. Do not introduce external knowledge or assumptions.\n- Ensure a logical flow, strong argumentation, and critical thinking throughout the paper.\n- Each of the ${c} paragraphs should be well-developed and contribute significantly to the overall paper.\n`:`You are a research assistant. Based *only* on the provided research summaries and their citations, and considering the original query and recent conversation, write a concise and informative research paper.\nOriginal Query: "${t}"\nRecent Conversation Context:\n${r}\n\nThis is a LITE research task. The paper should be well-structured and informative, consisting of ${c} paragraphs. Each paragraph must contain at least 10 sentences of meticulous research, and must contain thorough in-text citations.\n\n- Structure:\n    - Introduction (1 paragraph): Briefly introduce the topic based on the query and state the main points or findings from the summaries.\n    - Body (3-5 paragraphs, fitting the ${c} total): Discuss the key findings from the summaries. Each paragraph should focus on a distinct theme or piece of information. Synthesize information from multiple sources where appropriate.\n    - Conclusion (1 paragraph): Briefly summarize the main takeaways.\n- Synthesize findings from the summaries into a coherent narrative.\n- Do *not* refer to "Source 1", "Source 2", etc. Integrate information smoothly.\n- The final output should be ONLY the research paper text. A separate list of works cited will be compiled later using the provided citation data. Do not include a "References" or "Bibliography" section in your response.\n- Adhere strictly to the information within the provided summaries. Do not introduce external knowledge.\n- Ensure a clear and logical flow.\n`;const h={model:s,messages:[{role:"system",content:u},{role:"user",content:`Research Materials (Summaries & Citations):\n\n${n}`}],stream:!0,options:{temperature:o?.4:.6}};let d="";const m=await ollama.chat(h);for await(const e of m)d+=e.message.content,(d.length%500<(e.message.content?.length||0)||e.done)&&i({status:"Generating Final Report",detail:"Writing paper...",progress:d.length,partialPaper:d.substring(0,2e3)+"...",isFinal:!1});l=parse(d.trim())[1],i({status:"Generated Research Paper",detail:"Paper generation complete.",isFinal:!1})}catch(e){console.error("Error generating paper:",e),i({status:"Error Generating Paper",error:e.message,isFinal:!1}),l=`Error generating research paper: ${e.message}`}}else l="No valid summaries were generated to create a research paper.",i({status:"Skipped Paper Generation",detail:"No summaries available.",isFinal:!1});if(e.length>0){const t=(new Date).toLocaleDateString("en-US",{year:"numeric",month:"short",day:"numeric"});u=e.map((e=>e.citation?.trim())).filter((e=>e&&""!==e)).map((e=>{const a=String(e);return`- ${a}${a.includes("Accessed")?"":` (Accessed ${t})`}`})).join("\n"),u.trim()||(u="No valid citations were found in the processed data.")}else u="No citations were generated as no summaries were available.";return{researchPaper:l,citations:u}}export async function deepResearch(e,t,a=3,n=2,r=[],i=new Set,s=(e=>console.log(JSON.stringify(e))),o=1,l=!1,u,c,h,d=null){const m=u||(l?void 0:"qwen2.5vl:3b"),g=c||(l?void 0:"qwen2.5vl:3b"),p=h||(l?void 0:"qwen2.5vl:3b");s({status:"Starting Research Level",detail:`Query: "${e.substring(0,70)}...", Depth: ${o}/${n}, Breadth: ${a}, HeavyDuty: ${l}`,isFinal:!1});let f,y=e;if(o>1&&r.length>0){const t=r.slice(-Math.min(3,r.length)).map((e=>e.summary)).join("\n");y=`Original goal: "${e}". Prior findings from ${Math.min(3,r.length)} sources: "${t.substring(0,200)}...". Now, explore related sub-topics, alternative viewpoints, or delve deeper into specific aspects mentioned. Generate queries for this next level of investigation.`}try{f=await generateSerpQueries(y,t,a,s,9,m)}catch(i){if(s({status:"Failed",error:`Halting research: SERP generation failure: ${i.message}`,isFinal:1===o}),1===o){const i=await generateFinalReport(r,e,t,a,n,s,p,l);return void s({status:"Complete",finalResult:i,isFinal:!0})}f=[]}f&&0!==f.length||s({status:"No SERP Queries Generated for this Level",detail:"Proceeding with existing data or concluding.",isFinal:!1});const $=pLimit(3),v=[];if(f&&f.length>0){const a=f.map(((a,n)=>{const r=`(Lvl:${o}, Q:${n+1}/${f.length})`;return processSingleSerpQuery(a,e,t,i,$,s,r,g,d,l)}));(await Promise.all(a)).forEach((e=>{e&&e.length>0&&v.push(...e)}))}const w=[...r,...v];if(o<n)return s({status:"Starting Deeper Research Level",detail:`Moving to Depth ${o+1}`,isFinal:!1}),deepResearch(e,t,Math.max(1,Math.ceil(a/(l?1.2:1.5))),n,w,i,s,o+1,l,u,c,h,d);{s({status:"Max Depth Reached",detail:`Total processed items for report: ${w.length}`,isFinal:!1});const r=new Map;w.forEach((e=>{e&&e.url&&r.set(e.url,e)}));const i=Array.from(r.values()),o=await generateFinalReport(i,e,t,a,n,s,p,l);return void s({status:"Complete",finalResult:o,isFinal:!0,heavyDutyResearchId:l?d:void 0})}}