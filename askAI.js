import{Ollama}from"ollama";import{getFetchResults,getSearchResults}from"./interactWithInternet.js";import{deepResearch}from"./mainDeepResearch.js";import{geoLocate}from"./geoLocator.js";const ollama=new Ollama,MODE_NORMAL=0,MODE_DEEP_THINK=1,MODE_SEARCH=2,MODE_SEARCH_DT=3,MODE_DEEP_RESEARCH_LITE=4,MODE_YOUTUBE_TRANSCRIPT=5,MODE_AUTO=6,MODE_HEAVY_DUTY_DEEP_RESEARCH=7,DEFAULT_HEAVY_DUTY_THINKING_MODEL="deepseek-r1:1.5b";export function parse(e){const n=e.split(/<\/?think>/);return 1===n.length?["",n[0]]:[n[1]||"",n[2]||""]}function getAllMatches(e,n){const t=[];let r;for(;null!==(r=n.exec(e));)t.push(r);return t}async function search(e,n,t,r=5,o=!1,s="qwen2.5vl:3b"){var a=0,i=[],l=[];const c=n.map((e=>`${e.role}: ${e.content.substring(0,100)}`)).join("\n");t({thinking:"Determining optimal number of search queries...",response:"",isFinal:!1});var u=await generateNumber(`Based on the query "${e}" and conversation context:\n${c}\nHow many search queries (1-3) are needed? Respond with a number.`,s);let h=parseInt(u,10);for((isNaN(h)||h<=0||h>3)&&(h=Math.max(1,Math.min(n.length>2?2:1,3))),t({thinking:`AI suggested ${h} search queries.`,response:"",isFinal:!1});a<r;){var m=await generateQueries(h,e,n,(e=>t({...e,isFinal:!1})),s);if(!m||0===m.length)return t({thinking:"Failed to generate search queries. Aborting search.",response:"Could not generate search terms.",isFinal:!1}),i;var d=m.map((e=>getSearchResults(e))),g=await Promise.all(d);let c=!1;if(g.forEach((async e=>{if(e)for(var n of e)if(!l.includes(n.link)){l.push(n.link);let e=!0;if(o)try{var t=new URL(n.link).hostname;t.endsWith(".org")||t.endsWith(".gov")||t.endsWith(".edu")||(e=!1)}catch(n){e=!1}e&&(a++,i.push(n),c=!0)}})),l.length>5*r&&!c&&i.length<r){t({thinking:"Found many URLs but few are passing filters or are new. Breaking search early.",response:"",isFinal:!1});break}if(i.length>=r)break}if(t({thinking:`Collected ${i.length} search results. Deciding whether to read content...`,response:"",isFinal:!1}),!await generateBoolean(`Prompt: ${e}\nContext:\n${c}\nSearch results (titles/snippets):\n${JSON.stringify(i.slice(0,5).map((e=>({title:e.title,snippet:e.snippet}))))}\nShould I read full content of links (1 for yes, 0 for no)?`,s))return t({thinking:"AI decided not to read through links. Using snippets.",response:"",isFinal:!1}),i;if(0===l.length)return t({thinking:"No URLs found to read.",response:"",isFinal:!1}),i;t({thinking:`AI is reading up to ${Math.min(l.length,5)} URLs. This may take a while...`,response:"",isFinal:!1});const p=l.slice(0,5).map((async(n,r)=>{t({thinking:`Fetching and summarizing URL ${r+1}/${Math.min(l.length,5)}: ${n}`,response:"",isFinal:!1});const o=await getFetchResults(n);return o&&!o.startsWith("Error:")?summarize(o,4,e,s):`Could not retrieve or summarize content from ${n}.`}));var E=await Promise.all(p);let f=0;for(var y=0;y<i.length&&f<E.length;y++)l.includes(i[y].link)&&l.indexOf(i[y].link)<5&&(i[y].fullArticle=E[f++]);return t({thinking:`Finished reading and summarizing ${f} URLs.`,response:"",isFinal:!1}),i}async function determineBestMode(e,n,t,r="qwen2.5vl:3b",o=.3){t({thinking:"Auto Mode: Analyzing prompt to determine best mode...",response:"",isFinal:!1});let s="Conversation History:\n";n.slice(-5).forEach((e=>{s+=`${e.role}: ${e.content.substring(0,200)}${e.content.length>200?"...":""}\n`})),s+=`\nLatest User Prompt: "${e}"\n\n`;const a=/(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:watch\?v=|embed\/|v\/)|youtu\.be\/)([a-zA-Z0-9_-]{11})/.test(e);let i=`Based on the conversation history and the latest user prompt, choose the best mode to respond.\nAvailable modes:\n0: Normal - General conversation, direct answers, creative tasks.\n1: Deep Think - For complex questions requiring nuanced, structured thought before responding.\n2: Search - When external, up-to-date information or web search is clearly needed.\n3: Search + DT - Combines web search with deep thinking for complex searches.\n4: Deep Research Lite - For standard research on a topic, generating a concise paper.\n7: Heavy Duty Deep Research - For extensive, PhD-level research on a topic, very detailed paper, more depth.\n5: YouTube Transcript - If the prompt includes a YouTube link and asks to process its content.\n\nConsider user's intent. If YouTube link + process intent, choose 5.\nIf current data needed, choose 2 or 3.\nFor broad, comprehensive, detailed research, choose 7. For standard research, choose 4.\nFor complex reasoning, choose 1. Otherwise, default to 0.\n\nRespond with ONLY the integer code (e.g., "0", "1", "2", "3", "4", "5", "7") and a brief one-sentence reason on the next line.\nExample:\n2\nThe user is asking for the current weather.\n\n${s}`;a&&(i+="\nA YouTube link was detected. Consider mode 5.\n");try{const e=await ollama.chat({model:r,messages:[{role:"user",content:i}],stream:!1,options:{temperature:o}}),n=e.message?.content?.trim()||"0\nDefaulting to Normal mode.",[s,a]=n.split("\n",2);let l=parseInt(s,10);const c=[MODE_NORMAL,MODE_DEEP_THINK,MODE_SEARCH,MODE_SEARCH_DT,MODE_DEEP_RESEARCH_LITE,MODE_YOUTUBE_TRANSCRIPT,MODE_HEAVY_DUTY_DEEP_RESEARCH];!isNaN(l)&&c.includes(l)||(t({thinking:`Auto Mode: Failed to determine mode reliably (got '${s}'). Defaulting to Normal.`,response:"",isFinal:!1}),l=MODE_NORMAL);const u={id:l,name:{[MODE_NORMAL]:"Normal",[MODE_DEEP_THINK]:"Deep Think",[MODE_SEARCH]:"Search",[MODE_SEARCH_DT]:"Search + DT",[MODE_DEEP_RESEARCH_LITE]:"Deep Research Lite",[MODE_YOUTUBE_TRANSCRIPT]:"YouTube Transcript",[MODE_HEAVY_DUTY_DEEP_RESEARCH]:"Heavy Duty Deep Research"}[l]||"Unknown",reason:a||"No reason provided."};return t({thinking:`Auto Mode: Determined mode to be ${u.name}. Reason: ${u.reason}`,response:"",status:"Mode Determined",determinedModeInfo:u,isFinal:!1}),u}catch(e){return console.error("Error in determineBestMode:",e),t({thinking:`Auto Mode: Error determining mode: ${e.message}. Defaulting to Normal.`,response:"",isFinal:!1}),{id:MODE_NORMAL,name:"Normal",reason:"Error in auto-detection."}}}async function generateNumber(e,n="qwen2.5vl:3b"){try{const t=await ollama.chat({model:n,messages:[{role:"system",content:"Respond with only a single number. Example User: 'Count to 3' You: '3'"},{role:"user",content:e}],stream:!1,options:{temperature:.2}}),r=(t.message?.content?.trim()??"").match(/\d+/);return r?r[0]:""}catch(e){return console.error("Error in generateNumber:",e),""}}async function generateBoolean(e,n="qwen2.5vl:3b"){try{const t=await ollama.chat({model:n,messages:[{role:"system",content:"Respond with only '1' for true or '0' for false. Example User: 'Is water wet?' You: '1'"},{role:"user",content:e}],stream:!1,options:{temperature:.1}}),r=t.message?.content?.trim()??"";return"1"===r||"0"!==r&&(!(!r.toLowerCase().includes("true")&&!r.toLowerCase().includes("yes"))||(r.toLowerCase().includes("false")||r.toLowerCase().includes("no"),!1))}catch(e){return console.error("Error in generateBoolean:",e),!1}}async function summarize(e,n,t,r="qwen2.5vl:3b"){if(!e||!t||"number"!=typeof n||n<1)return console.error("Invalid input to summarize function."),"Error: Invalid input for summarization.";try{const o=await ollama.chat({model:r,messages:[{role:"system",content:`The user will give you website content. Extract the main content and provide a ${n}-sentence summary relevant to the query: "${t}". Respond only with the summary. If not relevant, say: "No relevant information found."`},{role:"user",content:e.substring(0,12e3)}],stream:!1});return o.message?.content?.trim()??"Summarization failed or produced no content."}catch(e){return console.error("Error during summarization:",e),"Error during summarization."}}async function generateQueries(e,n,t,r,o="qwen2.5vl:3b"){let s=`Generating ${e} search queries for: "${n.substring(0,50)}..."`;const a=t.slice(-3).map((e=>`${e.role}: ${e.content.substring(0,100)}`)).join("\n");r({thinking:s,response:""});try{let t=(await ollama.chat({model:o,messages:[{role:"system",content:`Based on the following conversation history and the latest user prompt, generate exactly ${e} SERP query(s). Output each query on its own line, formatted as: searchUp("query"). Example: searchUp("history of AI"). Do not include bullet points, JSON, or any other text.\n\nConversation History (last few turns):\n${a}`},{role:"user",content:`Latest User Prompt: ${n}`}],stream:!1})).message.content;t=t.replace(/p\('"/g,'("').replace(/'\)"/g,'")');const i=`${s}\nAI raw output: ${t.substring(0,200)}${t.length>200?"...":""}`;r({thinking:i,response:""});const l=getAllMatches(t,/(?:Search|search)(?:Up|_up|up|_Up)\("([^"]+)"\)/g).map((e=>e[1].trim())).filter((e=>e));if(0===l.length){r({thinking:`${i}\nNo valid queries found. Retrying or using fallback.`,response:""});const n=getAllMatches(t,/"([^"]+)"/g).map((e=>e[1].trim())).filter((e=>e&&e.length>3));return n.length>0?(r({thinking:`Used fallback to extract queries: ${n.slice(0,e).join(", ")}`,response:""}),n.slice(0,e)):(r({thinking:"Query generation failed to produce results in the correct format.",response:""}),[])}return r({thinking:`Generated queries: ${l.join(", ")}`,response:""}),l.slice(0,e)}catch(e){return console.error("Error generating queries:",e),r({thinking:`Error generating queries: ${e.message}`,response:""}),[]}}async function askAI(e,n,t,r=null,o=null,s=2,a=3,i=null,l=null,c=[],u,h=null,m=null,d=null){if(u){var g=await geoLocate(u),p=g.ipCity+", "+g.ipRegionName+", "+g.ipCountryName;if("user"==n[n.length-1].role&&!n[n.length-1].content.startsWith("I am located in "))await generateBoolean("Please tell me if the following prompt requires location or not (1 for yes, 0 for no). It should require location if it's something like weather, finding restraunts near me, or anything related to location. Here is the prompt: \n\n"+n[n.length-1].content)?(console.log("Location is important for this prompt: "+n[n.length-1].content),n[n.length-1].content="I am located in "+p+". \n\n"+n[n.length-1].content):console.log("Location is not important for this prompt: "+n[n.length-1].content)}let E="You are a helpful AI assistant...";i&&""!==i.trim()&&(E=i);let f=r,y="number"==typeof o?o:.7;if(6===e){const e=c.length>0?c[c.length-1].content:"",h=await determineBestMode(e,c,t,f||"qwen2.5vl:3b",y);return await askAI(h.id,n,t,r,o,s,a,i,l,c,u,null,m,d)}let D=r||"deepseek-r1:1.5b";const w=[{role:"system",content:E},...n].map((e=>"string"!=typeof e.content?{...e,content:String(e.content||"")}:e)).filter((e=>""!==e.content.trim()));if(1!==w.length||"system"!==w[0].role){if(e===MODE_NORMAL||e===MODE_DEEP_THINK||e===MODE_YOUTUBE_TRANSCRIPT)return await askAIStream(w,false,t,f,y);if(e===MODE_SEARCH||e===MODE_SEARCH_DT){const e=n[n.length-1].content;var R=await search(e,c,(e=>t({...e,isFinal:!1})),5,!1,f);const r=[{role:"system",content:E},...c.slice(0,-1),{role:"user",content:"Based on the following research, please answer my query.\n\nResearch Results:\n"+JSON.stringify(R.map((e=>({title:e.title,link:e.link,snippet:e.snippet,summary:e.fullArticle||""}))),null,2)},{role:"user",content:"My original query was: "+e}];return await askAIStream(r,false,t,f,y)}if(e===MODE_DEEP_RESEARCH_LITE||e===MODE_HEAVY_DUTY_DEEP_RESEARCH){if(n.length<1)return void t({status:"Error",detail:"Not enough context for deep research.",error:"Please provide a query.",isFinal:!0});const r=n[n.length-1].content;let o=e===MODE_HEAVY_DUTY_DEEP_RESEARCH,i=o?s+1:s,l=[],u=new Set,g=1,p=null;o&&(p=h?.heavyDutyResearchId||`${d}-${m}`),h&&o?(l=h.initialProcessedData||[],u=new Set(h.visitedUrls||[]),g=h.currentDepthToStartFrom||1,t({status:"Resuming Heavy Duty Research",detail:`Resuming from depth ${g}, with ${l.length} items. ID: ${p}`,isFinal:!1,heavyDutyResearchId:p})):o&&t({status:"Starting Heavy Duty Research",detail:`ID: ${p}`,isFinal:!1,heavyDutyResearchId:p});try{return void await deepResearch(r,c,a,i,l,u,t,g,o,o?D:void 0,o?D:void 0,o?D:void 0,p)}catch(e){return console.error("Deep research failed:",e),void t({status:"Error",detail:"Deep research process encountered an error.",error:e.message,isFinal:!0})}}}else t({thinking:"",response:"Hello! How can I assist you today?",isFinal:!0})}async function askAIStream(e,n,t,r,o){let s="",a="",i="";r||(r=n?"deepseek-r1:1.5b":"qwen2.5vl:3b"),o="number"==typeof o?o:.7;try{const l=await ollama.chat({model:r,messages:e,stream:!0,options:{temperature:o}});for await(const e of l)s+=e.message.content,n?([a,i]=parse(s),t({thinking:a,response:i,isFinal:!1})):(i=s,t({thinking:"",response:i,isFinal:!1}));t({thinking:a,response:i,isFinal:!0})}catch(e){console.error(`Error streaming from Ollama model ${r}:`,e),t({thinking:a||"Error during AI response generation.",response:i||`Sorry, I encountered an error with model ${r}: ${e.message}`,isFinal:!0})}}export{askAI};