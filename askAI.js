import{Ollama}from"ollama";import{getFetchResults,getSearchResults}from"./interactWithInternet.js";import{deepResearch}from"./mainDeepResearch.js";import{geoLocate}from"./geoLocator.js";const ollama=new Ollama,MODE_NORMAL=0,MODE_DEEP_THINK=1,MODE_SEARCH=2,MODE_SEARCH_DT=3,MODE_DEEP_RESEARCH_LITE=4,MODE_YOUTUBE_TRANSCRIPT=5,MODE_AUTO=6,MODE_HEAVY_DUTY_DEEP_RESEARCH=7,DEFAULT_HEAVY_DUTY_THINKING_MODEL="deepseek-r1:1.5b";export function parse(e){return e.startsWith("<think>")?(1==(e=(e=e.replace("<think>","")).split("</think>")).length&&e.push(""),e):["",e]}async function determineBestMode(e,t,n,r="qwen2.5vl:3b",o=.3){n({thinking:"Auto Mode: Analyzing prompt to determine best mode...",response:"",isFinal:!1});let s="Conversation History:\n";t.slice(-5).forEach((e=>{s+=`${e.role}: ${e.content.substring(0,200)}${e.content.length>200?"...":""}\n`})),s+=`\nLatest User Prompt: "${e}"\n\n`;const a=/(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:watch\?v=|embed\/|v\/)|youtu\.be\/)([a-zA-Z0-9_-]{11})/.test(e);let i=`Based on the conversation history and the latest user prompt, choose the best mode to respond.\nAvailable modes:\n0: Normal - General conversation, direct answers, creative tasks.\n1: Deep Think - For complex questions requiring nuanced, structured thought before responding.\n2: Search - When external, up-to-date information or web search is clearly needed.\n3: Search + DT - Combines web search with deep thinking for complex searches.\n4: Deep Research Lite - For standard research on a topic, generating a concise paper.\n7: Heavy Duty Deep Research - For extensive, PhD-level research on a topic, very detailed paper, more depth.\n5: YouTube Transcript - If the prompt includes a YouTube link and asks to process its content.\n\nConsider user's intent. If YouTube link + process intent, choose 5.\nIf current data needed, choose 2 or 3.\nFor broad, comprehensive, detailed research, choose 7. For standard research, choose 4.\nFor complex reasoning, choose 1. Otherwise, default to 0.\n\nRespond with ONLY the integer code (e.g., "0", "1", "2", "3", "4", "5", "7") and a brief one-sentence reason on the next line.\nExample:\n2\nThe user is asking for the current weather.\n\n${s}`;a&&(i+="\nA YouTube link was detected. Consider mode 5.\n");try{const e=await ollama.chat({model:r,messages:[{role:"user",content:i}],stream:!1,options:{temperature:o}}),t=e.message?.content?.trim()||"0\nDefaulting to Normal mode.",[s,a]=t.split("\n",2);let l=parseInt(s,10);const c=[MODE_NORMAL,MODE_DEEP_THINK,MODE_SEARCH,MODE_SEARCH_DT,MODE_DEEP_RESEARCH_LITE,MODE_YOUTUBE_TRANSCRIPT,MODE_HEAVY_DUTY_DEEP_RESEARCH];!isNaN(l)&&c.includes(l)||(n({thinking:`Auto Mode: Failed to determine mode reliably (got '${s}'). Defaulting to Normal.`,response:"",isFinal:!1}),l=MODE_NORMAL);const h={id:l,name:{[MODE_NORMAL]:"Normal",[MODE_DEEP_THINK]:"Deep Think",[MODE_SEARCH]:"Search",[MODE_SEARCH_DT]:"Search + DT",[MODE_DEEP_RESEARCH_LITE]:"Deep Research Lite",[MODE_YOUTUBE_TRANSCRIPT]:"YouTube Transcript",[MODE_HEAVY_DUTY_DEEP_RESEARCH]:"Heavy Duty Deep Research"}[l]||"Unknown",reason:a||"No reason provided."};return n({thinking:`Auto Mode: Determined mode to be ${h.name}. Reason: ${h.reason}`,response:"",status:"Mode Determined",determinedModeInfo:h,isFinal:!1}),h}catch(e){return console.error("Error in determineBestMode:",e),n({thinking:`Auto Mode: Error determining mode: ${e.message}. Defaulting to Normal.`,response:"",isFinal:!1}),{id:MODE_NORMAL,name:"Normal",reason:"Error in auto-detection."}}}async function search(e,t,n,r=5,o=!1,s="qwen2.5vl:3b"){var a=0,i=[],l=[];const c=t.map((e=>`${e.role}: ${e.content.substring(0,100)}`)).join("\n");n({thinking:"Determining optimal number of search queries...",response:"",isFinal:!1});var h=await generateNumber(`Based on the query "${e}" and conversation context:\n${c}\nHow many search queries (1-3) are needed? Respond with a number.`,s);let u=parseInt(h,10);for((isNaN(u)||u<=0||u>3)&&(u=Math.max(1,Math.min(t.length>2?2:1,3))),n({thinking:`AI suggested ${u} search queries.`,response:"",isFinal:!1});a<r;){var m=await generateQueries(u,e,t,(e=>n({...e,isFinal:!1})),s);if(!m||0===m.length)return n({thinking:"Failed to generate search queries. Aborting search.",response:"Could not generate search terms.",isFinal:!1}),i;var p=m.map((e=>getSearchResults(e))),d=await Promise.all(p);let c=!1;if(d.forEach((async e=>{if(e)for(var t of e)if(!l.includes(t.link)){l.push(t.link);let e=!0;if(o)try{var n=new URL(t.link).hostname;n.endsWith(".org")||n.endsWith(".gov")||n.endsWith(".edu")||(e=!1)}catch(t){e=!1}e&&(a++,i.push(t),c=!0)}})),l.length>5*r&&!c&&i.length<r){n({thinking:"Found many URLs but few are passing filters or are new. Breaking search early.",response:"",isFinal:!1});break}if(i.length>=r)break}if(n({thinking:`Collected ${i.length} search results. Deciding whether to read content...`,response:"",isFinal:!1}),!await generateBoolean(`Prompt: ${e}\nContext:\n${c}\nSearch results (titles/snippets):\n${JSON.stringify(i.slice(0,5).map((e=>({title:e.title,snippet:e.snippet}))))}\nShould I read full content of links or is the prompt already answered with the information/snipits? (1 for yes, 0 for no)?`,s))return n({thinking:"AI decided not to read through links. Using snippets.",response:"",isFinal:!1}),i;if(0===l.length)return n({thinking:"No URLs found to read.",response:"",isFinal:!1}),i;n({thinking:`AI is reading up to ${Math.min(l.length,5)} URLs. This may take a while...`,response:"",isFinal:!1});const g=l.slice(0,5).map((async(t,r)=>{n({thinking:`Fetching and summarizing URL ${r+1}/${Math.min(l.length,5)}: ${t}`,response:"",isFinal:!1});const o=await getFetchResults(t);return o&&!o.startsWith("Error:")?summarize(o,4,e,s):`Could not retrieve or summarize content from ${t}.`}));var E=await Promise.all(g);let f=0;for(var y=0;y<i.length&&f<E.length;y++)l.includes(i[y].link)&&l.indexOf(i[y].link)<5&&(i[y].fullArticle=E[f++]);return n({thinking:`Finished reading and summarizing ${f} URLs.`,response:"",isFinal:!1}),i}async function summarize(e,t,n,r="qwen2.5vl:3b"){if(!e||!n||"number"!=typeof t||t<1)return console.error("Invalid input to summarize function."),"Error: Invalid input for summarization.";try{const o=await ollama.chat({model:r,messages:[{role:"system",content:`The user will give you website content. Extract the main content and provide a ${t}-sentence summary relevant to the query: "${n}". Respond only with the summary. If not relevant, say: "No relevant information found."`},{role:"user",content:e.substring(0,12e3)}],stream:!1});return o.message?.content?.trim()??"Summarization failed or produced no content."}catch(e){return console.error("Error during summarization:",e),"Error during summarization."}}async function generateNumber(e,t="qwen2.5vl:3b"){try{const n=await ollama.chat({model:t,messages:[{role:"system",content:"Respond with only a single number. Example User: 'Count to 3' You: '3'"},{role:"user",content:e}],stream:!1,options:{temperature:.2}}),r=(n.message?.content?.trim()??"").match(/\d+/);return r?r[0]:""}catch(e){return console.error("Error in generateNumber:",e),""}}async function generateBoolean(e,t="qwen2.5vl:3b"){try{const n=await ollama.chat({model:t,messages:[{role:"system",content:"Respond with only '1' for true or '0' for false. Example User: 'Is water wet?' You: '1'"},{role:"user",content:e}],stream:!1,options:{temperature:.1}}),r=n.message?.content?.trim()??"";return"1"===r||"0"!==r&&(!(!r.toLowerCase().includes("true")&&!r.toLowerCase().includes("yes"))||(r.toLowerCase().includes("false")||r.toLowerCase().includes("no"),!1))}catch(e){return console.error("Error in generateBoolean:",e),!1}}function getAllMatches(e,t){const n=[];let r;for(;null!==(r=t.exec(e));)n.push(r);return n}async function generateQueries(e,t,n,r,o="qwen2.5vl:3b"){let s="",a=`Generating ${e} search queries for: "${t.substring(0,50)}..."`;const i=n.slice(-3).map((e=>`${e.role}: ${e.content.substring(0,100)}`)).join("\n");r({thinking:a,response:""});try{const n=await ollama.chat({model:o,messages:[{role:"system",content:`Based on the following conversation history and the latest user prompt, generate exactly ${e} SERP query(s). Output each query on its own line, formatted as: searchUp("query"). Example: searchUp("history of AI"). Do not include bullet points, JSON, or any other text.\n\nConversation History (last few turns):\n${i}`},{role:"user",content:`Latest User Prompt: ${t}`}],stream:!0});let l=a;for await(const e of n)s+=e.message.content.replace(/p\('"/g,'("').replace(/'\)"/g,'")'),l=`${a}\nAI raw output: ${s.substring(0,200)}${s.length>200?"...":""}`,r({thinking:l,response:""});const c=getAllMatches(s,/(?:Search|search)(?:Up|_up|up|_Up)\("([^"]+)"\)/g).map((e=>e[1].trim())).filter((e=>e));if(0===c.length){r({thinking:`${l}\nNo valid queries found. Retrying or using fallback.`,response:""});const t=getAllMatches(s,/"([^"]+)"/g).map((e=>e[1].trim())).filter((e=>e&&e.length>3));return t.length>0?(r({thinking:`Used fallback to extract queries: ${t.slice(0,e).join(", ")}`,response:""}),t.slice(0,e)):(r({thinking:"Query generation failed to produce results in the correct format.",response:""}),[])}return r({thinking:`Generated queries: ${c.join(", ")}`,response:""}),c.slice(0,e)}catch(e){return console.error("Error generating queries:",e),r({thinking:`Error generating queries: ${e.message}`,response:""}),[]}}async function askAI(e,t,n,r=null,o=null,s=2,a=3,i=null,l=null,c=[],h,u=null,m=null,p=null){if(h)if(await generateBoolean("Please tell me if location is important to the following prompt. 1 means yes and 0 means no. What I mean by important is I mean is it necessary to answer the prompt. For example, the prompt can ask what the weather is like around them, then location is important as weather differs by location. ANother one could be restaraunts near them, where location is important. However, an example of when location isn't important is when they ask for a coding prompt, since most of them don't really require location. Here is the prompt to analyze: \n\n"+t[t.length-1].content)){console.log("Location is important to the following prompt: "+t[t.length-1].content);var d=await geoLocate(h),g=d.ipCity+", "+d.ipRegionName+", "+d.ipCountryName;"user"!=t[t.length-1].role||t[t.length-1].content.startsWith("I am located in ")||(t[t.length-1].content="I am located in "+g+". \n\n"+t[t.length-1].content)}else console.log("Location is NOT important to the following prompt: "+t[t.length-1].content);let E="You are a helpful AI assistant...";i&&""!==i.trim()&&(E=i);let f=r,y="number"==typeof o?o:.7;if(6===e){const e=c.length>0?c[c.length-1].content:"",u=await determineBestMode(e,c,n,f||"qwen2.5vl:3b",y);return await askAI(u.id,t,n,r,o,s,a,i,l,c,h,null,m,p)}let D=r||"deepseek-r1:1.5b";const w=[{role:"system",content:E},...t].map((e=>"string"!=typeof e.content?{...e,content:String(e.content||"")}:e)).filter((e=>""!==e.content.trim()));if(1!==w.length||"system"!==w[0].role){if(e===MODE_NORMAL||e===MODE_DEEP_THINK||e===MODE_YOUTUBE_TRANSCRIPT)return await askAIStream(w,false,n,f,y);if(e===MODE_SEARCH||e===MODE_SEARCH_DT){const e=t[t.length-1].content;var R=await search(e,c,(e=>n({...e,isFinal:!1})),5,!1,f);const r=[{role:"system",content:E},...c.slice(0,-1),{role:"user",content:"Based on the following research, please answer my query.\n\nResearch Results:\n"+JSON.stringify(R.map((e=>({title:e.title,link:e.link,snippet:e.snippet,summary:e.fullArticle||""}))),null,2)},{role:"user",content:"My original query was: "+e}];return await askAIStream(r,false,n,f,y)}if(e===MODE_DEEP_RESEARCH_LITE||e===MODE_HEAVY_DUTY_DEEP_RESEARCH){if(t.length<1)return void n({status:"Error",detail:"Not enough context for deep research.",error:"Please provide a query.",isFinal:!0});const r=t[t.length-1].content;let o=e===MODE_HEAVY_DUTY_DEEP_RESEARCH,i=o?s+1:s,l=[],h=new Set,d=1,g=null;o&&(g=u?.heavyDutyResearchId||`${p}-${m}`),u&&o?(l=u.initialProcessedData||[],h=new Set(u.visitedUrls||[]),d=u.currentDepthToStartFrom||1,n({status:"Resuming Heavy Duty Research",detail:`Resuming from depth ${d}, with ${l.length} items. ID: ${g}`,isFinal:!1,heavyDutyResearchId:g})):o&&n({status:"Starting Heavy Duty Research",detail:`ID: ${g}`,isFinal:!1,heavyDutyResearchId:g});try{return void await deepResearch(r,c,a,i,l,h,n,d,o,o?D:void 0,o?D:void 0,o?D:void 0,g)}catch(e){return console.error("Deep research failed:",e),void n({status:"Error",detail:"Deep research process encountered an error.",error:e.message,isFinal:!0})}}}else n({thinking:"",response:"Hello! How can I assist you today?",isFinal:!0})}async function askAIStream(e,t,n,r,o){let s="",a="",i="";r||(r=t?"deepseek-r1:1.5b":"qwen2.5vl:3b"),o="number"==typeof o?o:.7;try{const t=await ollama.chat({model:r,messages:e,stream:!0,options:{temperature:o}});for await(const e of t)s+=e.message.content,[a,i]=parse(s),n({thinking:a,response:i,isFinal:!1});n({thinking:a,response:i,isFinal:!0})}catch(e){console.error(`Error streaming from Ollama model ${r}:`,e),n({thinking:a||"Error during AI response generation.",response:i||`Sorry, I encountered an error with model ${r}: ${e.message}`,isFinal:!0})}}export{askAI};